{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal commands\n",
    "\n",
    "# pip install nltk \n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries for simple pre-processing text\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/viddesh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/viddesh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/viddesh/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/viddesh/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"This is a sample text for demonstrating various text preprocessing techniques.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['This', 'is', 'a', 'sample', 'text', 'for', 'demonstrating', 'various', 'text', 'preprocessing', 'techniques', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase Tokens: ['this', 'is', 'a', 'sample', 'text', 'for', 'demonstrating', 'various', 'text', 'preprocessing', 'techniques', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "lowercase_tokens = [token.lower() for token in tokens]\n",
    "print(\"Lowercase Tokens:\", lowercase_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase Tokens: ['THIS', 'IS', 'A', 'SAMPLE', 'TEXT', 'FOR', 'DEMONSTRATING', 'VARIOUS', 'TEXT', 'PREPROCESSING', 'TECHNIQUES', '.']\n"
     ]
    }
   ],
   "source": [
    "# Uppercase\n",
    "uppercase_tokens = [token.upper() for token in tokens]\n",
    "print(\"Uppercase Tokens:\", uppercase_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (Stop Words Removed): ['sample', 'text', 'demonstrating', 'various', 'text', 'preprocessing', 'techniques', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(\"Filtered Tokens (Stop Words Removed):\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Removed Tokens: ['This', 'is', 'a', 'sample', 'text', 'for', 'demonstrating', 'various', 'text', 'preprocessing', 'techniques']\n"
     ]
    }
   ],
   "source": [
    "# Remove Noise (non-alphabetic characters)\n",
    "noise_removed_tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "noise_removed_tokens = [token for token in noise_removed_tokens if token]  # Remove empty strings\n",
    "print(\"Noise Removed Tokens:\", noise_removed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN'), ('for', 'IN'), ('demonstrating', 'VBG'), ('various', 'JJ'), ('text', 'JJ'), ('preprocessing', 'NN'), ('techniques', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Parts of Speech Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['thi', 'is', 'a', 'sampl', 'text', 'for', 'demonstr', 'variou', 'text', 'preprocess', 'techniqu', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens (NLTK): ['This', 'is', 'a', 'sample', 'text', 'for', 'demonstrating', 'various', 'text', 'preprocessing', 'technique', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmatized Tokens (NLTK):\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens (Spacy): ['this', 'be', 'a', 'sample', 'text', 'for', 'demonstrate', 'various', 'text', 'preprocesse', 'technique', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using Spacy\n",
    "doc = nlp(text)\n",
    "spacy_lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatized Tokens (Spacy):\", spacy_lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal commands\n",
    "\n",
    "# pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.58\n",
      "Recall: 0.75\n",
      "F1 Score: 0.65\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00         1\n",
      "     neutral       0.00      0.00      0.00         1\n",
      "    positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.56      0.67      0.60         4\n",
      "weighted avg       0.58      0.75      0.65         4\n",
      "\n",
      "\n",
      "Naive Bayes Prediction for 'I love this movie': positive\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes classifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Sample data (increased size)\n",
    "data = {\n",
    "    'text': [\n",
    "        'I love this movie', 'This movie is great', 'I hate this movie',\n",
    "        'This movie is not good', 'I enjoyed the movie', 'The movie was terrible',\n",
    "        'What a fantastic film', 'Not a fan of this movie', 'It was an okay movie',\n",
    "        'The film was superb', 'Loved the plot', 'Terrible acting', 'Great direction',\n",
    "        'The story was captivating', 'Worst movie ever', 'It was a wonderful experience',\n",
    "        'Not worth watching', 'Absolutely fantastic', 'Could have been better', 'Mediocre film'\n",
    "    ],\n",
    "    'label': [\n",
    "        'positive', 'positive', 'negative', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'neutral', 'positive', 'positive', 'negative',\n",
    "        'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'neutral', 'neutral'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and Labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Stratified Split to ensure each class is represented equally\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(strat_split.split(X, y))\n",
    "\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Create a pipeline that combines vectorization and the Naive Bayes classifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),            # Convert text to token counts\n",
    "    ('tfidf', TfidfTransformer()),          # Transform token counts to TF-IDF features\n",
    "    ('clf', MultinomialNB()),               # Multinomial Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))\n",
    "\n",
    "print()\n",
    "\n",
    "# Detailed Classification Report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "print()\n",
    "\n",
    "# Predict on new text\n",
    "new_text = \"I love this movie\"\n",
    "predicted_label_nb = text_clf.predict([new_text])\n",
    "print(f\"Naive Bayes Prediction for '{new_text}': {predicted_label_nb[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "Precision: 0.25\n",
      "Recall: 0.25\n",
      "F1 Score: 0.25\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "     neutral       0.00      0.00      0.00         1\n",
      "    positive       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.17      0.17         4\n",
      "weighted avg       0.25      0.25      0.25         4\n",
      "\n",
      "\n",
      "Naive Bayes Prediction for 'I hate this movie': negative\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree classifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Sample data (increased size)\n",
    "data = {\n",
    "    'text': [\n",
    "        'I love this movie', 'This movie is great', 'I hate this movie',\n",
    "        'This movie is not good', 'I enjoyed the movie', 'The movie was terrible',\n",
    "        'What a fantastic film', 'Not a fan of this movie', 'It was an okay movie',\n",
    "        'The film was superb', 'Loved the plot', 'Terrible acting', 'Great direction',\n",
    "        'The story was captivating', 'Worst movie ever', 'It was a wonderful experience',\n",
    "        'Not worth watching', 'Absolutely fantastic', 'Could have been better', 'Mediocre film'\n",
    "    ],\n",
    "    'label': [\n",
    "        'positive', 'positive', 'negative', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'neutral', 'positive', 'positive', 'negative',\n",
    "        'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'neutral', 'neutral'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and Labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Stratified Split to ensure each class is represented equally\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(strat_split.split(X, y))\n",
    "\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Create a pipeline that combines vectorization and the Decision Tree classifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),            # Convert text to token counts\n",
    "    ('tfidf', TfidfTransformer()),          # Transform token counts to TF-IDF features\n",
    "    ('clf', DecisionTreeClassifier(random_state=42)),  # Decision Tree classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))\n",
    "\n",
    "print()\n",
    "\n",
    "# Detailed Classification Report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "print()\n",
    "\n",
    "# Predict on new text\n",
    "new_text = \"I hate this movie\"\n",
    "predicted_label_nb = text_clf.predict([new_text])\n",
    "print(f\"Naive Bayes Prediction for '{new_text}': {predicted_label_nb[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis using textblob\n",
    "# pip install textblob\n",
    "# pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this movie!\n",
      "Polarity: 0.625, Subjectivity: 0.6\n",
      "Sentiment: positive\n",
      "\n",
      "Text: This movie is terrible...\n",
      "Polarity: -1.0, Subjectivity: 1.0\n",
      "Sentiment: negative\n",
      "\n",
      "Text: I enjoyed the movie very much.\n",
      "Polarity: 0.38, Subjectivity: 0.48\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Not a fan of this movie.\n",
      "Polarity: 0.0, Subjectivity: 0.0\n",
      "Sentiment: neutral\n",
      "\n",
      "Text: The plot was fantastic and the acting superb!\n",
      "Polarity: 0.4666666666666666, Subjectivity: 0.6333333333333333\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Worst movie ever.\n",
      "Polarity: -1.0, Subjectivity: 1.0\n",
      "Sentiment: negative\n",
      "\n",
      "Text: Absolutely fantastic!\n",
      "Polarity: 0.5, Subjectivity: 0.9\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Could have been better.\n",
      "Polarity: 0.5, Subjectivity: 0.5\n",
      "Sentiment: positive\n",
      "\n",
      "Text: It was an okay movie.\n",
      "Polarity: 0.5, Subjectivity: 0.5\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Loved the plot and the characters.\n",
      "Polarity: 0.7, Subjectivity: 0.8\n",
      "Sentiment: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_textblob_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    'I love this movie!',\n",
    "    'This movie is terrible...',\n",
    "    'I enjoyed the movie very much.',\n",
    "    'Not a fan of this movie.',\n",
    "    'The plot was fantastic and the acting superb!',\n",
    "    'Worst movie ever.',\n",
    "    'Absolutely fantastic!',\n",
    "    'Could have been better.',\n",
    "    'It was an okay movie.',\n",
    "    'Loved the plot and the characters.'\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis\n",
    "for text in texts:\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment\n",
    "    print(f\"Text: {text}\\nPolarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")\n",
    "    sentiment = get_textblob_sentiment(text)\n",
    "    print(f\"Sentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this movie!\n",
      "Scores: {'neg': 0.0, 'neu': 0.4, 'pos': 0.6, 'compound': 0.6696}\n",
      "Sentiment: positive\n",
      "\n",
      "Text: This movie is terrible...\n",
      "Scores: {'neg': 0.508, 'neu': 0.492, 'pos': 0.0, 'compound': -0.4767}\n",
      "Sentiment: negative\n",
      "\n",
      "Text: I enjoyed the movie very much.\n",
      "Scores: {'neg': 0.0, 'neu': 0.602, 'pos': 0.398, 'compound': 0.5106}\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Not a fan of this movie.\n",
      "Scores: {'neg': 0.282, 'neu': 0.718, 'pos': 0.0, 'compound': -0.2411}\n",
      "Sentiment: negative\n",
      "\n",
      "Text: The plot was fantastic and the acting superb!\n",
      "Scores: {'neg': 0.0, 'neu': 0.429, 'pos': 0.571, 'compound': 0.8398}\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Worst movie ever.\n",
      "Scores: {'neg': 0.672, 'neu': 0.328, 'pos': 0.0, 'compound': -0.6249}\n",
      "Sentiment: negative\n",
      "\n",
      "Text: Absolutely fantastic!\n",
      "Scores: {'neg': 0.0, 'neu': 0.193, 'pos': 0.807, 'compound': 0.6352}\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Could have been better.\n",
      "Scores: {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "Sentiment: positive\n",
      "\n",
      "Text: It was an okay movie.\n",
      "Scores: {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Loved the plot and the characters.\n",
      "Scores: {'neg': 0.0, 'neu': 0.562, 'pos': 0.438, 'compound': 0.5994}\n",
      "Sentiment: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    if compound >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    'I love this movie!',\n",
    "    'This movie is terrible...',\n",
    "    'I enjoyed the movie very much.',\n",
    "    'Not a fan of this movie.',\n",
    "    'The plot was fantastic and the acting superb!',\n",
    "    'Worst movie ever.',\n",
    "    'Absolutely fantastic!',\n",
    "    'Could have been better.',\n",
    "    'It was an okay movie.',\n",
    "    'Loved the plot and the characters.'\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis\n",
    "for text in texts:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    print(f\"Text: {text}\\nScores: {scores}\")\n",
    "    sentiment = get_vader_sentiment(text)\n",
    "    print(f\"Sentiment: {sentiment}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
