{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "# Data Preprocessing\n",
    "# Handling Missing Values\n",
    "# \n",
    "# 1\n",
    "# Objective: Learn how to handle missing values in a dataset.\n",
    "# Techniques: Mean/median imputation, dropping missing values.\n",
    "# Encoding Categorical Data\n",
    "\n",
    "\n",
    "# Mean imputation\n",
    "df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
    "\n",
    "# Median imputation\n",
    "df['column_name'].fillna(df['column_name'].median(), inplace=True)\n",
    "\n",
    "# Mode imputation\n",
    "df['column_name'].fillna(df['column_name'].mode()[0], inplace=True)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_column'] = label_encoder.fit_transform(df['original_column'])\n",
    "\n",
    "# Encoding categorical data\n",
    "import pandas as pd\n",
    "df = pd.get_dummies(df, columns=['categorical_column'])\n",
    "\n",
    "# categorical data\n",
    "ordinal_mapping = {'low': 1, 'medium': 2, 'high': 3}\n",
    "df['ordinal_column'] = df['ordinal_column'].map(ordinal_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category  encoded_category\n",
      "0        A                 0\n",
      "1        B                 1\n",
      "2        A                 0\n",
      "3        C                 2\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "# Objective: Learn how to encode categorical data.\n",
    "# Techniques: Label encoding, one-hot encoding.\n",
    "# Feature Scaling\n",
    "\n",
    "\n",
    "# Encoding Categorical data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'category': ['A', 'B', 'A', 'C']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_category'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   category_A  category_B  category_C\n",
      "0        True       False       False\n",
      "1       False        True       False\n",
      "2        True       False       False\n",
      "3       False       False        True\n"
     ]
    }
   ],
   "source": [
    "# One-Hot encoding\n",
    "# Example DataFrame\n",
    "data = {'category': ['A', 'B', 'A', 'C']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['category'])\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature1  feature2\n",
      "0 -1.341641 -1.341641\n",
      "1 -0.447214 -0.447214\n",
      "2  0.447214  0.447214\n",
      "3  1.341641  1.341641\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling (standard scaling)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example DataFrame with numerical features\n",
    "data = {'feature1': [10, 20, 30, 40], 'feature2': [1, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Standardization\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   feature1  feature2\n",
      "0        10         1\n",
      "1        20         2\n",
      "2        30         3\n",
      "3        40         4\n",
      "\n",
      "Scaled Data (Standardization):\n",
      "   feature1  feature2\n",
      "0 -1.341641 -1.341641\n",
      "1 -0.447214 -0.447214\n",
      "2  0.447214  0.447214\n",
      "3  1.341641  1.341641\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Objective: Learn how to scale features in a dataset.\n",
    "# Techniques: Standardization, normalization.\n",
    "# Data Normalization\n",
    "\n",
    "\n",
    "# Feature Scaling Techniques (standard scaler)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with numerical features\n",
    "data = {'feature1': [10, 20, 30, 40], 'feature2': [1, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nScaled Data (Standardization):\")\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   feature1  feature2\n",
      "0        10         1\n",
      "1        20         2\n",
      "2        30         3\n",
      "3        40         4\n",
      "\n",
      "Normalized Data:\n",
      "   feature1  feature2\n",
      "0  0.000000  0.000000\n",
      "1  0.333333  0.333333\n",
      "2  0.666667  0.666667\n",
      "3  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Normalization - Min Max\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "df_normalized = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(df_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   feature1  feature2\n",
      "0        10         1\n",
      "1        20         2\n",
      "2        30         3\n",
      "3        40         4\n",
      "4      1000       500\n",
      "\n",
      "Normalized Data (Min-Max Scaling):\n",
      "   feature1  feature2\n",
      "0  0.000000  0.000000\n",
      "1  0.010101  0.002004\n",
      "2  0.020202  0.004008\n",
      "3  0.030303  0.006012\n",
      "4  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "# Objective: Learn how to normalize data.\n",
    "# Techniques: Min-Max scaling, z-score normalization.\n",
    "# Handling Outliers\n",
    "\n",
    "# Min-Max Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with numerical features\n",
    "data = {'feature1': [10, 20, 30, 40, 1000], 'feature2': [1, 2, 3, 4, 500]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "df_normalized = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nNormalized Data (Min-Max Scaling):\")\n",
    "print(df_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   feature1  feature2\n",
      "0        10         1\n",
      "1        20         2\n",
      "2        30         3\n",
      "3        40         4\n",
      "4      1000       500\n",
      "\n",
      "Normalized Data (Z-Score Normalization):\n",
      "   feature1  feature2\n",
      "0 -0.538285 -0.507531\n",
      "1 -0.512652 -0.502506\n",
      "2 -0.487019 -0.497481\n",
      "3 -0.461387 -0.492456\n",
      "4  1.999343  1.999975\n"
     ]
    }
   ],
   "source": [
    "# Standard Scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "df_normalized = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nNormalized Data (Z-Score Normalization):\")\n",
    "print(df_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   values\n",
      "0    10.0\n",
      "1    20.0\n",
      "2    30.0\n",
      "3    40.0\n",
      "4    40.0\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "# Objective: Learn how to detect and handle outliers in a dataset.\n",
    "# Techniques: IQR method, z-score method.\n",
    "# TF-IDF\n",
    "# Text Vectorization using TF-IDF\n",
    "\n",
    "# IQR\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'values': [10, 20, 30, 40, 1000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = df['values'].quantile(0.25)\n",
    "Q3 = df['values'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Trim outliers\n",
    "df['values'] = df['values'].apply(lambda x: Q3 if x > (Q3 + 1.5 * IQR) else Q1 if x < (Q1 - 1.5 * IQR) else x)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Data:\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Optional: Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame (for better visualization)\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(\"TF-IDF Vectorized Data:\")\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Data:\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "# Objective: Learn how to use TF-IDF for text vectorization.\n",
    "# Techniques: TF-IDF transformation, feature extraction.\n",
    "# Cosine Similarity using TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Optional: Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame (for better visualization)\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(\"TF-IDF Vectorized Data:\")\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Convert cosine similarities to a DataFrame (for better visualization)\n",
    "df_cosine = pd.DataFrame(cosine_similarities, columns=range(1, len(documents) + 1), index=range(1, len(documents) + 1))\n",
    "\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(df_cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Data:\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "\n",
    "# Objective: Learn how to use TF-IDF to calculate cosine similarity between documents.\n",
    "# Techniques: TF-IDF vectorization, cosine similarity computation.\n",
    "# Naive Bayes Algorithm\n",
    "# Text Classification using Naive Bayes\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Optional: Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame (for better visualization)\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(\"TF-IDF Vectorized Data:\")\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity Matrix:\n",
      "          1         2         3         4\n",
      "1  1.000000  0.646926  0.307772  1.000000\n",
      "2  0.646926  1.000000  0.225240  0.646926\n",
      "3  0.307772  0.225240  1.000000  0.307772\n",
      "4  1.000000  0.646926  0.307772  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity matrix\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Convert cosine similarities to a DataFrame (for better visualization)\n",
    "df_cosine = pd.DataFrame(cosine_similarities, columns=range(1, len(documents) + 1), index=range(1, len(documents) + 1))\n",
    "\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(df_cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example dataset for text classification\n",
    "texts = [\"good movie\", \"not good movie\", \"very bad movie\", \"good movie indeed\", \"bad movie\"]\n",
    "labels = [1, 1, 0, 1, 0]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the texts\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "# Objective: Learn how to classify text using Naive Bayes.\n",
    "# Techniques: TF-IDF vectorization, Multinomial Naive Bayes, model evaluation.\n",
    "# Spam Detection using Naive Bayes\n",
    "\n",
    "# Naive Bayes\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
    "        \"Even my brother is not like to speak with me. They treat me like aids patent.\",\n",
    "        \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
    "        \"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\",\n",
    "        \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\"\n",
    "    ],\n",
    "    'label': [1, 0, 0, 1, 1]  # 1 for spam, 0 for not spam\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "# # Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Multinomial Naive Bayes Classifier\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/viddesh/Desktop/NLP/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 9\n",
    "# Objective: Learn how to use Naive Bayes for spam detection.\n",
    "# Techniques: TF-IDF vectorization, Multinomial Naive Bayes, model evaluation.\n",
    "# ID3 Algorithm\n",
    "# Decision Tree Classifier using ID3 Algorithm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
    "        \"Even my brother is not like to speak with me. They treat me like aids patent.\",\n",
    "        \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
    "        \"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\",\n",
    "        \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\"\n",
    "    ],\n",
    "    'label': [1, 0, 0, 1, 1]  # 1 for spam, 0 for not spam\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load example dataset (replace with your actual data)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "classifier = DecisionTreeClassifier(criterion=\"entropy\")  # ID3 uses entropy for information gain\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m tree_classifier \u001b[38;5;241m=\u001b[39m ID3DecisionTree()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Fit the model on training data\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtree_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[1;32m     71\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m, in \u001b[0;36mID3DecisionTree.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36mID3DecisionTree._grow_tree\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(y)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbincount(y)\u001b[38;5;241m.\u001b[39margmax()\n\u001b[1;32m     36\u001b[0m best_feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minformation_gain(X, y, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# 10\n",
    "# Objective: Learn how to implement the ID3 algorithm for classification.\n",
    "# Techniques: Decision tree training, entropy calculation, model evaluation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy_value\n",
    "    \n",
    "    def information_gain(self, X, y, feature_index):\n",
    "        entropy_before_split = self.entropy(y)\n",
    "        unique_values = np.unique(X[:, feature_index])\n",
    "        weighted_entropy_after_split = 0\n",
    "        \n",
    "        for value in unique_values:\n",
    "            subset_indices = np.where(X[:, feature_index] == value)[0]\n",
    "            subset_entropy = self.entropy(y[subset_indices])\n",
    "            weighted_entropy_after_split += (len(subset_indices) / len(y)) * subset_entropy\n",
    "        \n",
    "        information_gain_value = entropy_before_split - weighted_entropy_after_split\n",
    "        return information_gain_value\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y):\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return np.unique(y)[0]\n",
    "        \n",
    "        if X.shape[1] == 0:\n",
    "            return np.bincount(y).argmax()\n",
    "        \n",
    "        best_feature = np.argmax([self.information_gain(X, y, i) for i in range(X.shape[1])])\n",
    "        tree = {best_feature: {}}\n",
    "        \n",
    "        unique_values = np.unique(X[:, best_feature])\n",
    "        for value in unique_values:\n",
    "            subset_indices = np.where(X[:, best_feature] == value)[0]\n",
    "            subtree = self._grow_tree(X[subset_indices], y[subset_indices])\n",
    "            tree[best_feature][value] = subtree\n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
    "    \n",
    "    def _predict_single(self, x, tree):\n",
    "        feature = list(tree.keys())[0]\n",
    "        value = x[feature]\n",
    "        subtree = tree[feature][value]\n",
    "        \n",
    "        if isinstance(subtree, dict):\n",
    "            return self._predict_single(x, subtree)\n",
    "        else:\n",
    "            return subtree\n",
    "\n",
    "# Example usage:\n",
    "# Assume X_train, y_train are your training features and labels\n",
    "# Assume X_test is your test features\n",
    "\n",
    "# Create an instance of ID3DecisionTree\n",
    "tree_classifier = ID3DecisionTree()\n",
    "\n",
    "# Fit the model on training data\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = tree_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m attribute_names \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayTennis\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Train the ID3 decision tree\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mid3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Predict a new sample\u001b[39;00m\n\u001b[1;32m     85\u001b[0m new_sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutlook\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSunny\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHumidity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrong\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "Cell \u001b[0;32mIn[20], line 46\u001b[0m, in \u001b[0;36mid3\u001b[0;34m(X, y, attribute_names, depth, max_depth)\u001b[0m\n\u001b[1;32m     43\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m X[X[:, best_attr] \u001b[38;5;241m==\u001b[39m value]\n\u001b[1;32m     44\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m y[X[:, best_attr] \u001b[38;5;241m==\u001b[39m value]\n\u001b[0;32m---> 46\u001b[0m     subtree \u001b[38;5;241m=\u001b[39m \u001b[43mid3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattribute_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_attr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     tree[best_attr_name][value] \u001b[38;5;241m=\u001b[39m subtree\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "Cell \u001b[0;32mIn[20], line 35\u001b[0m, in \u001b[0;36mid3\u001b[0;34m(X, y, attribute_names, depth, max_depth)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Choose the attribute with the highest information gain\u001b[39;00m\n\u001b[1;32m     34\u001b[0m best_attr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(gains)\n\u001b[0;32m---> 35\u001b[0m best_attr_name \u001b[38;5;241m=\u001b[39m \u001b[43mattribute_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_attr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Create the tree root with the best attribute\u001b[39;00m\n\u001b[1;32m     38\u001b[0m tree \u001b[38;5;241m=\u001b[39m {best_attr_name: {}}\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "\n",
    "def entropy(s):\n",
    "    counts = Counter(s)\n",
    "    probabilities = [count / len(s) for count in counts.values()]\n",
    "    return -sum(p * log2(p) for p in probabilities if p > 0)\n",
    "\n",
    "def information_gain(y, x):\n",
    "    total_entropy = entropy(y)\n",
    "    values, counts = np.unique(x, return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / len(y)) * entropy(y[x == values[i]]) for i in range(len(values)))\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "def id3(X, y, attribute_names, depth=0, max_depth=None):\n",
    "    # If all the target values are the same, return that value\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return y[0]\n",
    "\n",
    "    # If no features are left to split on, return the most common target value\n",
    "    if len(attribute_names) == 0 or (max_depth is not None and depth >= max_depth):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    # Calculate information gains for all attributes\n",
    "    gains = [information_gain(y, X[:, i]) for i in range(X.shape[1])]\n",
    "    \n",
    "    # Check if gains is empty (no valid attributes left)\n",
    "    if not any(gains):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    # Choose the attribute with the highest information gain\n",
    "    best_attr = np.argmax(gains)\n",
    "    best_attr_name = attribute_names[best_attr]\n",
    "\n",
    "    # Create the tree root with the best attribute\n",
    "    tree = {best_attr_name: {}}\n",
    "    \n",
    "    # Recursively create the tree for each value of the best attribute\n",
    "    values = np.unique(X[:, best_attr])\n",
    "    for value in values:\n",
    "        X_subset = X[X[:, best_attr] == value]\n",
    "        y_subset = y[X[:, best_attr] == value]\n",
    "        \n",
    "        subtree = id3(X_subset, y_subset,\n",
    "                      [attr for i, attr in enumerate(attribute_names) if i != best_attr],\n",
    "                      depth + 1, max_depth)\n",
    "        \n",
    "        tree[best_attr_name][value] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "def predict(tree, sample):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    \n",
    "    attr = next(iter(tree))\n",
    "    value = sample.get(attr, None)  # Get the value from the sample or None if not found\n",
    "    if value is None or value not in tree[attr]:\n",
    "        # Handle missing attribute value by returning a default prediction\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    subtree = tree[attr][value]\n",
    "    return predict(subtree, sample)\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "X = df.drop(columns='PlayTennis').values\n",
    "y = df['PlayTennis'].values\n",
    "attribute_names = df.drop(columns='PlayTennis').columns.tolist()\n",
    "\n",
    "# Train the ID3 decision tree\n",
    "tree = id3(X, y, attribute_names)\n",
    "\n",
    "# Predict a new sample\n",
    "new_sample = {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'High', 'Wind': 'Strong'}\n",
    "prediction = predict(tree, new_sample)\n",
    "print(f\"Prediction for {new_sample}: {prediction}\")\n",
    "\n",
    "# Print the tree\n",
    "import pprint\n",
    "pprint.pprint(tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
